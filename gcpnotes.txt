bq load -> VERY VERY IMPORTANT 
https://cloud.google.com/bigquery/docs/information-schema-datasets



array -> same data type [REPEATED]
struct -> diff or same data types, but data was 'related together'
 [RECORD]
marks in 10 subjects

if marks-> int16, [ m1, m2....m10] 
if name -> string, ['shaktiman', 'pikachu'...] 

(name, subject, marks!)
-> ARRAY of STRUCT
 -> STRUCT can have more arrays or STRUCTs as children!


data: { 
field_name: [ 10,20,30]
}

UNNEST(), ARRAY_AGG, -> unnesting and nesting (along with struct)
diff b/w COUNT and DISTINCT COUNT -> integrity check 
COUNTIF -> IF statement -> isolate problems in advance (slide 10 in notes)
besides NULL, BLANK is also possible 

EL -> when data was in usable format
ELT -> when data was same, but transformations were diff
ETL -> in all other cases (specially use-case specific) 

Any production dataset where the transformation can be expressed in SQL
Raw data-> GCS -> BigQuery (ELT)

100% sure data is clean 
Raw data -> Big Query (EL) 


Data consistency demo:

1)
CREATE OR REPLACE TABLE pikachu.monkey AS

SELECT
123 as height,
'Tomato' as name;

SELECT * from pikachu.monkey; 

2) removed data 
DELETE FROM pikachu.monkey WHERE true;

3) consistency check with HAVING clause
SELECT COUNT(*) as rowcount
from pikachu.monkey
HAVING 
  IF(rowcount > 0, true, ERROR(FORMAT('%t count resulted in bad rows', rowcount)));
  
SELECT * from pikachu.monkey WHERE name = 'Tomato';

1.2Pb/s -> almost half of internet speed
NRT latency (-> 0ms) 

Data could have resided in DATA LAKE ITSELF and PROCESSED BY OTHER
TOOLS WITHOUT NEEDING TO BE ACTUALLY TRANSPORTED!


Structured v/s Unstructured Data
Google maps-> Name, Lat, Long, { } 
Drive from A to B -> structured for me!
Fly from A to B -> ? UNSTRUCTURED!!! 

SQL v/s MongoDB
Relation v/s Non Relational


Data Catalogue -> Data's ACCESS CONTROL LIST

-> yester -> SCHEMA-> DC could've prevented unwanted data from coming in
-> today -> Using TAGS (METADATA for tables) 
		or SECURITY (DLP api) 

-> Why? -> UNIFIED DATA DISCOVERY
	- inbuilt Data Loss Prevention API

abc@gmail.com -> PII -> a***@g***.com 

-> Row level data access
	-> IAM : Dataset -> all the tables 
	
	-> Data at Rest-> encryption 
	-> Data in Motion-> TLS 



Cheaper VMs-> Preemtible VMs -> CLUSTER 

20 potatoes 
1-> 1 person, who eats 5 potatoes per min-> CPU
	->small instructions (add ax, mov bx, cx int 21h..)
2-> a person, who eats 1 potato per min, but 10 such people-> GPU
	-> slow ops but HUGE THROUGHPUT! 
	-> MUCH MORE DATA is processed! 

who will finish first?
-> 2nd!


replication factor -> 3

biggest size -> 1 GB 

biggest_size_of_data X replication factor + 4 GB 

Spark -> RAM biggest_size_of_data X replication factor + 8 GB 
HDFS -> disk space  (biggest_size_of_data X replication factor+ 100s)

GCS -> data lake

Data Proc -> ONLY compute cluster
GCS -> persistent 

UDF -> JavaScript 

https://cloud.google.com/dataproc/docs/concepts/jobs/life-of-a-job

https://cloud.google.com/dataproc/docs/concepts/jobs/restartable-jobs

https://cloud.google.com/dataproc/docs/guides/dataproc-images

https://cloud.google.com/dataproc/docs/concepts/workflows/using-yamls

https://cloud.google.com/bigtable/docs/performance

https://services.google.com/fh/files/misc/next-19-google-cloud-customer-voices-digital-book.pdf

https://cloud.google.com/bigquery/docs/gis-intro

Data Catalog -> 

Programing:
010101-> mov ax,10-> A,B -> C->C++->Java

Browsers-> HTML/CSS/JS-> what to do, machine figured out the best
	way to do it 

Imperitive vs Declarative 
- immediate execution                 - build a DAG
	manage heap + stack 
prediction = inputs * weights + bias



                             I             D
a=1         a=?             1              ok
b=2         b=?             2              ok
c=a+b       c=?             3              ok 
print(c)    ??              print(3)       calc->print(c)

Lazy Evaluation -> Declarative Languages (PySpark, Scala, TF...) 
(slow)

Imperative/immediate/Eager Evaluation -> imp
	-> Near Real time responses
	-> apps, mobile, sensors/devices/actuators 

Variety of data 
IoT-> Telemetry, Operational, Lifecycle 
(same device) -> JSON, NoSQL, BSON


- how many devices?
	-> Bulk-> VOLUME-> SQL no no
	-> not made for volume-> SQL

-> MongoDB -> BigTable (large volume)
small devices -> Firestore 

images -> gcs/data lake, URI-> nosql (BigTable/Firestore) + metadata


BigQuery-> Stream and JOBS are DIFF

Streams are infinite-> Streaming API of BigQuery
-> CLient Py 

Big Table-> Each key/pair value < 10 MB

Data size < 1 TB -> Big Table won't be able to perform

Cluster based COLOSSUS file system
	- TABLETS-> pointer table pointing to DISK storage 
	- each VM 300GB of FS 

Metadata -> TRIE 

Where not to use BigTable:
- Transactions (use Spanner or CloudSQL)
- Structure (use BigQuery)

Tablet
Pointers-> Address_id


Strings, arrays, List, , Disks, Tables.... 

-> release pointer, create a new one 

HDD-> 500 QPS
SSD-> 10,000 QPS

response time = reading delay + processing delay


Key Visualizer: (automatically generates usage graphs based on 
row keys)
1) last 24 hours-> at least 30 GB of data 
OR
2) last 24 hours-> avg (all reads+all writes) > 10k rows per second


Autoscaling -> CLUSTERS 

COLOSSUS FS-> More machines you have-> faster queries as result 
Autoscaling -> min -> 10 nodes 



Kubeflow pipelines-> 

kfctl-> cmd tool 

sep pipeline-> TRAINING 

pipeline for production 


export API_KEY=AIzaSyBFrd78Q_yp2XV48m095JEKWjcLZLJw9jI


Certificate -> CRT + Private Token -> BULK Auth 

Client Apps => API Keys (Server side apps) 

export KUBEFLOW_USERNAME=student-01-627f1136f77a@qwiklabs.net
export KUBEFLOW_PASSWORD=npwZNL74BCK2


Coursera + linux academy 

-> documentation, architecture, place in a pipline 

- ML on GCP
- DL on GCP 


